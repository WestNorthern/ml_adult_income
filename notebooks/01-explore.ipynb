{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229fe023",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.data import load_adult_from_openml, clean_data, split_data, save_splits\n",
    "\n",
    "df_raw = load_adult_from_openml()\n",
    "print(\"Raw:\", df_raw.shape)\n",
    "\n",
    "df = clean_data(df_raw)\n",
    "print(\"Clean:\", df.shape)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(df)\n",
    "print(\"Splits â†’\", \n",
    "      \"train:\", X_train.shape, \n",
    "      \"val:\", X_val.shape, \n",
    "      \"test:\", X_test.shape)\n",
    "\n",
    "save_splits(X_train, X_val, X_test, y_train, y_val, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b0b70",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from modeling import build_model_pipeline, evaluate_model\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(max_depth=5),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"HistGB\": HistGradientBoostingClassifier(random_state=42),\n",
    "}\n",
    "\n",
    "pipelines = {name: build_model_pipeline(clf) for name, clf in models.items()}\n",
    "\n",
    "# Train models\n",
    "for name, pipe in pipelines.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    pipe.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e4d3f1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "for name, pipe in pipelines.items():\n",
    "    metrics = evaluate_model(pipe, X_val, y_val)\n",
    "    metrics[\"Model\"] = name\n",
    "    results.append(metrics)\n",
    "\n",
    "df_results = pd.DataFrame(results).set_index(\"Model\")\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40611e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_results.index, df_results[\"accuracy\"], marker='o', label='Accuracy')\n",
    "plt.plot(df_results.index, df_results[\"precision\"], marker='o', label='Precision')\n",
    "plt.title(\"Accuracy vs Precision Across Models\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3b977b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from modeling import evaluate_by_group\n",
    "\n",
    "race_results = {}\n",
    "sex_results = {}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    race_results[name] = evaluate_by_group(pipe, X_val, y_val, group_col=\"race\")\n",
    "    sex_results[name] = evaluate_by_group(pipe, X_val, y_val, group_col=\"sex\")\n",
    "\n",
    "race_results[\"HistGB\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c482e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_group_comparison(results_dict, metric, group_name):\n",
    "    \"\"\"\n",
    "    Create side-by-side bar plots comparing the given metric across models and groups.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    df_combined = pd.concat(\n",
    "        {model: df[metric] for model, df in results_dict.items()}, axis=1\n",
    "    )\n",
    "    df_combined.plot(kind=\"bar\", ax=ax)\n",
    "    \n",
    "    ax.set_title(f\"{metric.title()} by {group_name.title()} and Model\")\n",
    "    ax.set_ylabel(metric.title())\n",
    "    ax.set_xlabel(group_name.title())\n",
    "    ax.legend(title=\"Model\")\n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example: Precision by race\n",
    "plot_group_comparison(race_results, \"precision\", \"race\")\n",
    "\n",
    "# Example: Recall by sex\n",
    "plot_group_comparison(sex_results, \"recall\", \"sex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125732ba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Combine group metrics for each model into a single DataFrame\n",
    "def collect_group_metrics(results_dict, group_name):\n",
    "    records = []\n",
    "    for model_name, df in results_dict.items():\n",
    "        for group_value in df.index:\n",
    "            row = df.loc[group_value].to_dict()\n",
    "            row.update({\n",
    "                \"Model\": model_name,\n",
    "                group_name.title(): group_value\n",
    "            })\n",
    "            records.append(row)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "race_report_df = collect_group_metrics(race_results, \"race\")\n",
    "sex_report_df = collect_group_metrics(sex_results, \"sex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167947c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_disparity(df, group_col, metric):\n",
    "    disparity_df = (\n",
    "        df.groupby(\"Model\")[metric]\n",
    "        .agg(lambda col: col.max() - col.min())\n",
    "        .reset_index(name=f\"{metric}_disparity\")\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.bar(disparity_df[\"Model\"], disparity_df[f\"{metric}_disparity\"])\n",
    "    ax.set_title(f\"{metric.title()} Disparity Across {group_col.title()} Groups\")\n",
    "    ax.set_ylabel(\"Disparity (Max - Min)\")\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_disparity(race_report_df, \"race\", \"precision\")\n",
    "plot_disparity(sex_report_df, \"sex\", \"recall\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
